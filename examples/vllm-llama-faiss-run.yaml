# Example llama-stack configuration for vLLM on RHEL, Meta Llama 3.1 Instruct + FAISS (RAG)
# 
# Notes:
# - You will need to serve Llama 3.1 Instruct on a vLLM instance
#
version: 2
image_name: vllm-llama-faiss-config

apis:
- agents
- inference
- vector_io
- tool_runtime
- safety

models:
- model_id: meta-llama/Llama-3.1-8B-Instruct
  provider_id: vllm
  model_type: llm
  provider_model_id: null
- model_id: sentence-transformers/all-mpnet-base-v2
  metadata:
      embedding_dimension: 768
  model_type: embedding
  provider_id: sentence-transformers
  provider_model_id: /home/USER/embedding_models/all-mpnet-base-v2
  
providers:
  inference:
  - provider_id: sentence-transformers
    provider_type: inline::sentence-transformers
    config: {}
  - provider_id: vllm
    provider_type: remote::vllm
    config:
      url: http://localhost:8000/v1/
      api_token: key

  agents:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      persistence_store:
        type: sqlite
        db_path: .llama/distributions/ollama/agents_store.db
      responses_store:
        type: sqlite
        db_path: .llama/distributions/ollama/responses_store.db

  safety:
  - provider_id: llama-guard
    provider_type: inline::llama-guard
    config:
      excluded_categories: []

  vector_io:
  - provider_id: rhel-db
    provider_type: inline::faiss
    config:
      kvstore:
        type: sqlite
        db_path: /home/USER/vector_dbs/rhel_index/faiss_store.db
        namespace: null

  tool_runtime:
  - provider_id: rag-runtime
    provider_type: inline::rag-runtime
    config: {}

tool_groups:
- provider_id: rag-runtime
  toolgroup_id: builtin::rag
  args: null
  mcp_endpoint: null

vector_dbs:
- embedding_dimension: 768
  embedding_model: sentence-transformers/all-mpnet-base-v2
  provider_id: rhel-db
  vector_db_id: rhel-docs